{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import decimal                         # decide on given number how many decimal points to keep for values in array\n",
    "from tqdm import tqdm_notebook as tqdm # check progress\n",
    "from tabulate import tabulate          # nice print of table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_multiple_distance(p_array, p):\n",
    "    return np.sqrt(np.square(p_array - p).sum(axis=1))\n",
    "\n",
    "def gaussian_kernel(distance, param):\n",
    "    return np.exp(-0.5 * np.square(distance/param['sigma'])) / np.power(param['sigma'] * np.sqrt(2*np.pi), param['dim'])\n",
    "\n",
    "def normalize(data):\n",
    "    # mitigate outliers, drop extreme 1% from each side\n",
    "    data_slice = scipy.stats.trimboth(data, proportiontocut=0.01) \n",
    "    mean = np.mean(data_slice, axis=0)[np.newaxis]\n",
    "    std = np.std(data_slice, axis=0).max()\n",
    "    data_normed = (data - mean) / std\n",
    "    return data_normed, mean, std\n",
    "\n",
    "def denormalize(data, mean, std):\n",
    "    return data * std + mean \n",
    "\n",
    "def meanShiftClustering(data, window, stop_criteria, param):\n",
    "    \"\"\"Find clusters using Mean-shift algorithm.\n",
    "    \n",
    "    Args: \n",
    "        data: numpy array\n",
    "        window: float, how far to look for neighbor points\n",
    "        stop_criteria: dict, 'epsilon' float distance boundary representing convergence, 'max_iter' int number of iterations making sure we are not stuck forever\n",
    "        param: dict, parameters for kernel, now it holds 'sigma': float; and 'dim': int, dimensionality of input data points.\n",
    "    \n",
    "    Return:\n",
    "        cluster_ids: numpy array, id of cluster to which data point has been assigned\n",
    "        cluster_centers: numpy array, coordinates of estimated cluster centers\n",
    "        converged: bool, whether process converged (hitted 'epsilon' criteria) or not (hitted 'max_iter' criteria)\n",
    "    \"\"\"\n",
    "    def mean_shift_iteration(data, point, window, distance_multiple_func, kernel, param):\n",
    "        # find neighbor points\n",
    "        distances = distance_multiple_func(data, point)\n",
    "        neighbors_idx = np.where(distances < window)[0]\n",
    "        \n",
    "        # select neighbors\n",
    "        neighbors = data[neighbors_idx]\n",
    "        neighbors_distances = distances[neighbors_idx]\n",
    "        \n",
    "        # compute summation coefficients\n",
    "        weights = kernel(neighbors_distances, param)\n",
    "        \n",
    "        # get centroid as a weighted average\n",
    "        centroid = (weights[:, np.newaxis] * neighbors).sum(axis=0) / weights.sum()\n",
    "        return centroid\n",
    "\n",
    "    distance_multiple_func = euclidean_multiple_distance\n",
    "    kernel = gaussian_kernel\n",
    "\n",
    "    max_iter = stop_criteria['max_iter']\n",
    "    epsilon = stop_criteria['epsilon']\n",
    "\n",
    "    converged = False\n",
    "    data = data.copy()\n",
    "    data, mean, std = normalize(data)  # so that we can keep hyperparams more-less in the same range\n",
    "    param['dim'] = data.shape[1]\n",
    "    \n",
    "    for i in tqdm(range(max_iter)):\n",
    "        # apply mean shift to all points at the same time (shift all points to mean)\n",
    "        data_new = np.stack([mean_shift_iteration(data, point, window, distance_multiple_func, kernel, param) for point in data])\n",
    "        \n",
    "        # check what the largest shift was\n",
    "        max_shift = np.max(distance_multiple_func(data_new, data))\n",
    "        data = data_new\n",
    "    \n",
    "        # if largest shift was small enough - converged - terminate algorithm\n",
    "        if max_shift < epsilon:\n",
    "            converged = True\n",
    "            break\n",
    "    \n",
    "    # round to the one digit more coerce level than epsilon has digits after comma\n",
    "    # that is to make sure all points of a cluster have exactly the same value\n",
    "    rounding_digits = -1 - decimal.Decimal(str(epsilon)).as_tuple().exponent\n",
    "    data = np.round(data, rounding_digits)\n",
    "    \n",
    "    # return to the original scale\n",
    "    data = denormalize(data, mean, std)\n",
    "    \n",
    "    # find cluster center positions\n",
    "    cluster_centers = np.array(list(set(tuple(p) for p in data)))\n",
    "    \n",
    "    # assign points to clusters \n",
    "    cluster_ids = np.array([i for point in data for i, centroid in enumerate(cluster_centers) if np.all(point == centroid)])\n",
    "    \n",
    "    return cluster_ids, cluster_centers, converged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data scaling, meaningful results can be obtained for multiple datasets with hyperparams as is below. \n",
    "\n",
    "Except for window (size), which should be fine-tuned for each dataset separately, because clusters have different relative sizes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_criteria = {'max_iter': 100, 'epsilon': 1e-8}\n",
    "param = {'sigma': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth clusters: 2\n"
     ]
    }
   ],
   "source": [
    "gt_cluster_centers = np.loadtxt('data/g2-gt-txt/g2-8-60-gt.txt')\n",
    "data = np.loadtxt('data/g2-txt/g2-8-60.txt')\n",
    "window = 1.25 * np.sqrt(data.shape[1])  # later multiplicator is to account for distance scaling with rise of dimensionality\n",
    "\n",
    "print(\"Number of ground truth clusters:\", gt_cluster_centers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1456c9016449378e9b0edc8b6b7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged: True\n",
      "Number of clusters: 2\n"
     ]
    }
   ],
   "source": [
    "cluster_ids, cluster_centers, converged = meanShiftClustering(data, window, stop_criteria, param)\n",
    "print(\"Converged:\",  converged)\n",
    "print(\"Number of clusters:\", cluster_centers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  x0    x1    x2    x3    x4    x5    x6    x7    dx0    dx1    dx2    dx3    dx4    dx5    dx6    dx7\n",
      "----  ----  ----  ----  ----  ----  ----  ----  -----  -----  -----  -----  -----  -----  -----  -----\n",
      " 504   500   503   500   504   508   504   504      4      0      3      0      4      8      4      4\n",
      " 597   597   600   597   596   596   596   593     -3     -3      0     -3     -4     -4     -4     -7\n"
     ]
    }
   ],
   "source": [
    "rounding_digits = -decimal.Decimal(gt_cluster_centers.reshape(-1)[0]).as_tuple().exponent\n",
    "cluster_centers_rounded = np.round(cluster_centers, rounding_digits)\n",
    "\n",
    "# find closest cluster to the ground truth from calculated\n",
    "matches = np.array([cluster_centers_rounded[np.argmin(euclidean_multiple_distance(cluster_centers_rounded, gt))] for gt in gt_cluster_centers])\n",
    "\n",
    "# calculate differences in positions between linked calculated clusters and ground truth clusters \n",
    "diffs = matches - gt_cluster_centers\n",
    "\n",
    "# xi - coordinates of linked calculated clusters, dxi - differences with ground truth \n",
    "print(tabulate(np.concatenate((matches, diffs), axis=1).tolist(), headers=[*[f'x{i}' for i in range(data.shape[1])], *[f'dx{i}' for i in range(data.shape[1])]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cluster_id    count\n",
      "------------  -------\n",
      "           0     1021\n",
      "           1     1027\n"
     ]
    }
   ],
   "source": [
    "idx, count = np.unique(cluster_ids, return_counts=True)\n",
    "print(tabulate(np.stack((idx, count), axis=1), headers=['cluster_id', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ground truth clusters: 15\n"
     ]
    }
   ],
   "source": [
    "gt_cluster_centers = np.loadtxt('data/s-originals/s1-cb.txt')\n",
    "data = np.loadtxt('data/s/s1.txt')\n",
    "window = 0.275 * np.sqrt(data.shape[1])\n",
    "\n",
    "print(\"Number of ground truth clusters:\", gt_cluster_centers.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c424d1db5d4755a68ed11485406ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converged: True\n",
      "Number of clusters: 16\n"
     ]
    }
   ],
   "source": [
    "cluster_ids, cluster_centers, converged = meanShiftClustering(data, window, stop_criteria, param)\n",
    "print(\"Converged:\",  converged)\n",
    "print(\"Number of clusters:\", cluster_centers.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we have one excessive cluster, keep that in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x0      x1    dx0    dx1\n",
      "------  ------  -----  -----\n",
      "606294  571377   1966  -3002\n",
      "802684  319603    776   1221\n",
      "416341  787615    -42   1411\n",
      "823377  729719    606  -2315\n",
      "851692  158250    699    377\n",
      "335509  561760  -3077  -1777\n",
      "167922  347818  -1352   -756\n",
      "617676  400922  -1583   3251\n",
      "246121  846950   5050   2526\n",
      "322476  162581    675  -2738\n",
      "141611  557544   2118    192\n",
      "506510  175900  -2275   1100\n",
      "398885  405346    -49   1204\n",
      "858793  547291  -2065   1232\n",
      "672172  861813  -2193   1349\n"
     ]
    }
   ],
   "source": [
    "rounding_digits = -decimal.Decimal(gt_cluster_centers.reshape(-1)[0]).as_tuple().exponent\n",
    "cluster_centers_rounded = np.round(cluster_centers, rounding_digits)\n",
    "matches = np.array([cluster_centers_rounded[np.argmin(euclidean_multiple_distance(cluster_centers_rounded, gt))] for gt in gt_cluster_centers])\n",
    "diffs = matches - gt_cluster_centers\n",
    "print(tabulate(np.concatenate((matches, diffs), axis=1).tolist(), headers=[*[f'x{i}' for i in range(data.shape[1])], *[f'dx{i}' for i in range(data.shape[1])]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  cluster_id    count\n",
      "------------  -------\n",
      "           0      351\n",
      "           1      341\n",
      "           2      345\n",
      "           3      339\n",
      "           4      311\n",
      "           5      314\n",
      "           6        1\n",
      "           7      297\n",
      "           8      318\n",
      "           9      329\n",
      "          10      340\n",
      "          11      334\n",
      "          12      353\n",
      "          13      350\n",
      "          14      327\n",
      "          15      350\n"
     ]
    }
   ],
   "source": [
    "idx, count = np.unique(cluster_ids, return_counts=True)\n",
    "print(tabulate(np.stack((idx, count), axis=1), headers=['cluster_id', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So cluster 6 is a single isolated point, it (and overly small clusters in general) can be reassigned to the closest clusters, discarded altogether, etc during post processing stage. \n",
    "\n",
    "I will leave it be for now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
